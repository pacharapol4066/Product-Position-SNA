{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IS Proof of concept - DRMABS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import math\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "today = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import bigrams\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pythainlp\n",
    "import pythainlp\n",
    "from pythainlp import Tokenizer\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus.common import thai_words\n",
    "from pythainlp import sent_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from pythainlp.util import dict_trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install selenium\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files definition\n",
    "root_path = 'Datasource'\n",
    "urls_past_file,comment_past_file = [],[]\n",
    "urls_past_file.append(root_path+\"/urls_meiji.csv\")\n",
    "urls_past_file.append(root_path+\"/urls_dutchmilk.csv\")\n",
    "urls_past_file.append(root_path+\"/urls_dairyhome.csv\")\n",
    "urls_past_file.append(root_path+\"/urls_chokchai.csv\")\n",
    "urls_past_file.append(root_path+\"/urls_foremost.csv\")\n",
    "\n",
    "comment_past_file.append(root_path+\"/comment_DDCF.csv\")\n",
    "\n",
    "# จำนวนกระทู้ที่ต้องการให้ดึงใน 1 นาที\n",
    "postNum = 7\n",
    "postNumSleep = math.ceil(60/postNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### กรณียังไม่เคยดึง URL ที่มีการพูดถึง Keyword ใน Pantip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of URL from search page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls = 'https://pantip.com/search?q=นมเมจิ'\n",
    "#urls = 'https://pantip.com/search?q=นมดัชมิลล์'\n",
    "#urls = 'https://pantip.com/search?q=นมแดรี่โฮม'\n",
    "#urls = 'https://pantip.com/search?q=นมโชคชัย'\n",
    "#urls = 'https://pantip.com/search?q=นมโฟร์โมสต์'\n",
    "\n",
    "#driver = webdriver.Chrome(executable_path='selenium/chromedriver.exe')\n",
    "#driver.get(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scroll down to get all data (Crawler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#while True:\n",
    "#    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#    time.sleep(2)\n",
    "#    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#    if new_height == last_height:\n",
    "#        break\n",
    "#    last_height = new_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all URLs (web scraping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#content=driver.page_source\n",
    "#soup=BeautifulSoup(content,'lxml')\n",
    "#urlList = []\n",
    "# ใช้ class_ เพื่อดึง div class จำเพาะ เพื่อไม่ให้ข้อความอื่นที่ไม่ต้องการปนมา\n",
    "#for div in soup.find_all(\"div\", class_=lambda value: value and value==\"title col-md-12\"):\n",
    "#    for a in div.find_all(\"a\", class_=lambda value: value and value==\"datasearch-in\", href=True):\n",
    "#        urlList.append(str(a['href']).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save URLs into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_urls = pd.DataFrame(urlList)\n",
    "#df_urls.insert(1,'Retrived date',today)\n",
    "#df_urls.columns = ['URLs','Retrived date']\n",
    "#df_urls.to_csv(urls_past_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### กรณีเคยเก็บ URL แล้ว ให้เริ่มตรงนี้ได้เลย เพื่อไม่ให้ต้องทำ Request บ่อยๆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop for scaping by URLs from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# เรียก URL Meiji ไม่ซ้ำกัน 1000 URLs\n",
    "df_URLRead = pd.read_csv(urls_past_file[0])\n",
    "df_URLRead = pd.DataFrame(df_URLRead['URLs'].unique())\n",
    "df_URLRead.columns = ['URLs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# เรียก URL จากแบรนด์รองทั้งหมด ดึงพร้อมกัน ไม่ซ้ำกัน 601 URLs\n",
    "df_URLRead1 = pd.read_csv(urls_past_file[1])\n",
    "df_URLRead2 = pd.read_csv(urls_past_file[2])\n",
    "df_URLRead3 = pd.read_csv(urls_past_file[3])\n",
    "df_URLRead4 = pd.read_csv(urls_past_file[4])\n",
    "dfURL = pd.concat([df_URLRead1,df_URLRead2,df_URLRead3,df_URLRead4])\n",
    "dfURL = pd.DataFrame(dfURL['URLs'].unique())\n",
    "dfURL.columns = ['URLs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URLs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://pantip.com/topic/39797060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://pantip.com/topic/39337438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://pantip.com/topic/39990528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://pantip.com/topic/39455240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://pantip.com/topic/38874550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                URLs\n",
       "0  https://pantip.com/topic/39797060\n",
       "1  https://pantip.com/topic/39337438\n",
       "2  https://pantip.com/topic/39990528\n",
       "3  https://pantip.com/topic/39455240\n",
       "4  https://pantip.com/topic/38874550"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfURL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URLs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>https://pantip.com/topic/31477185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>https://pantip.com/topic/31346514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>https://pantip.com/topic/31198026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>https://pantip.com/topic/31124656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>https://pantip.com/topic/30910132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 URLs\n",
       "74  https://pantip.com/topic/31477185\n",
       "75  https://pantip.com/topic/31346514\n",
       "76  https://pantip.com/topic/31198026\n",
       "77  https://pantip.com/topic/31124656\n",
       "78  https://pantip.com/topic/30910132"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find Dup URL from meiji\n",
    "df_dupURL = pd.merge(dfURL,df_URLRead,how='inner')\n",
    "df_dupURL.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_URLs = dfURL[~dfURL.URLs.isin(set(df_dupURL.URLs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(522, 1)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_URLs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urls = list(df_URLs['URLs'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "522"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "startResult = 0\n",
    "endResult = len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing URL no.  1  on  2020-10-27 13:54:33.920930\n"
     ]
    }
   ],
   "source": [
    "comments_list = []\n",
    "for cnt,au in enumerate(all_urls[startResult:endResult],0):\n",
    "    today = datetime.now()\n",
    "    print('Processing URL no. ',cnt+1,' on ',today)\n",
    "    driver = webdriver.Chrome(executable_path='selenium/chromedriver.exe')\n",
    "    driver.get(au)\n",
    "    content=driver.page_source\n",
    "    soup=BeautifulSoup(content,'lxml')\n",
    "    df_comments = pd.DataFrame()\n",
    "    try:\n",
    "        for div in soup.find_all(\"div\", class_=lambda value: value and value==\"display-post-story\"):\n",
    "            comments_list.append([au,str(div.text).strip()])\n",
    "        df_comments = pd.DataFrame(comments_list)\n",
    "        df_comments.insert(2,'Retrived date',today)\n",
    "        df_comments.columns = ['URLs', 'text', 'Retrived date']\n",
    "        df_comments.to_csv(comment_past_file[0], index=False, header=False, mode='a')\n",
    "    except:\n",
    "        print('กระทู้ถูกลบ')\n",
    "    comments_list.clear()\n",
    "    del df_comments\n",
    "    time.sleep(postNumSleep)\n",
    "    driver.quit()\n",
    "    clear_output(wait=True)\n",
    "    #จำกัดจำนวนกระทู้ สำหรับ Debug\n",
    "    #if cnt == 3:\n",
    "    #    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
