{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IS Proof of concept - DRMABS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "today = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "# pip install selenium\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files definition\n",
    "root_path = 'Datasource'\n",
    "urls_past_file = root_path+\"/all_dairy_urls.csv\"\n",
    "comment_past_file = root_path+\"/comment_DDCF.csv\"\n",
    "\n",
    "# จำนวนกระทู้ที่ต้องการให้ดึงใน 1 นาที\n",
    "postNum = 7\n",
    "postNumSleep = math.ceil(60/postNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect MongoDB\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "mydb = myclient[\"NIDA_PPSN_PRD\"]\n",
    "col_comment = mydb[\"NIDA_PPSN_SCRAPE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### กรณียังไม่เคยดึง URL ที่มีการพูดถึง Keyword ใน Pantip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of URL from search page"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "keywords = ['นมเมจิ','นมmeiji','โฟร์โมสต์','โฟโมส','โฟโมต','โฟโม้ส'\n",
    "            ,'ดัชมิลล์','ดัชมิล','ดัชมิว','dutchmill','dutchmilk'\n",
    "            ,'นมไทยเดนมาร์ค','นมไทยเดนมาร์ก','วัวแดง','mmilk','m milk','เอ็มมิลค์'\n",
    "            ,'นมโชคชัย','อืมม์มิลค์','ummmilk','นมแดรี่โฮม','นมเดรี่โฮม','dairyhome','นมพาสเจอร์ไรส์']\n",
    "urls = 'https://pantip.com/search?q='\n",
    "url_by_KW = [urls+k for k in keywords]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "url_by_KW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scroll down to get all data (Crawler)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for url in url_by_KW:\n",
    "    print('Search term = ',url)\n",
    "    driver = webdriver.Chrome(executable_path='selenium/chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    content=driver.page_source\n",
    "    soup=BeautifulSoup(content,'lxml')\n",
    "    urlList = []\n",
    "    # ใช้ class_ เพื่อดึง div class จำเพาะ เพื่อไม่ให้ข้อความอื่นที่ไม่ต้องการปนมา\n",
    "    for div in soup.find_all(\"div\", class_=lambda value: value and value==\"title col-md-12\"):\n",
    "        for a in div.find_all(\"a\", class_=lambda value: value and value==\"datasearch-in\", href=True):\n",
    "            urlList.append(str(a['href']).strip())\n",
    "    df_urls = pd.DataFrame(urlList)\n",
    "    df_urls.insert(1,'Retrived date',today)\n",
    "    df_urls.columns = ['URLs','Retrived date']\n",
    "    df_urls.to_csv(urls_past_file, mode='a', index=False)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### กรณีเคยเก็บ URL แล้ว ให้เริ่มตรงนี้ได้เลย เพื่อไม่ให้ต้องทำ Request บ่อยๆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop for scaping by URLs from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfURL = pd.read_csv(urls_past_file)\n",
    "#dfURL = pd.concat([df_URLRead,df_URLRead1,df_URLRead2,df_URLRead3,df_URLRead4])\n",
    "dfURL.drop_duplicates(keep='last',inplace=True,ignore_index=True)\n",
    "dfURL.columns = ['URLs','Retrived_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfURL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ListofDict_InsertMany(df):\n",
    "    listofdict = []\n",
    "    for c,lx in enumerate(df.URLs,0):\n",
    "        info = {\n",
    "                \"URLs\": lx,\n",
    "                \"headline\" : df.headline[c],\n",
    "                \"text\": df.text[c],\n",
    "                \"article_date\" : datetime.strptime(df.article_date[c], '%m/%d/%Y %H:%M:%S'),\n",
    "                \"Retrived_date\": df.Retrived_date[c]\n",
    "        }\n",
    "        listofdict.append(info)\n",
    "    return listofdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startResult = 4880\n",
    "endResult = 5152\n",
    "\n",
    "# 0-1000 Complete\n",
    "# 1001-2000 Complete\n",
    "# 2001-3000 Complete\n",
    "# 3001-4000 Complete\n",
    "# 4001-5000\n",
    "# 5001-5152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comments_list,time_list,error_list = [],[],[]\n",
    "\n",
    "# รันรอบต่อไปให้ comment del many ออกด้วย\n",
    "#col_comment.delete_many({})\n",
    "\n",
    "for cnt,au in enumerate(dfURL.URLs[startResult:endResult],0):\n",
    "    today = datetime.now()\n",
    "    print('Processing URL no. ',startResult+cnt,' on ',today)\n",
    "    driver = webdriver.Chrome(executable_path='selenium/chromedriver.exe')\n",
    "    driver.get(au)\n",
    "    content=driver.page_source\n",
    "    soup=BeautifulSoup(content,'lxml')\n",
    "    try:\n",
    "        div1 = soup.find(\"div\", class_=lambda value: value and value==\"display-post-status-leftside\")\n",
    "        headline_t = div1.find(\"h2\", class_=lambda value: value and value==\"display-post-title\")\n",
    "        for abbr in soup.find_all(\"abbr\", class_=lambda value: value and value==\"timeago\"):\n",
    "            time_list.append(abbr['data-utime'])\n",
    "        for div3 in soup.find_all(\"div\", class_=lambda value: value and value==\"display-post-story\"):\n",
    "            if len(div3['class'])<2:\n",
    "                comments_list.append([au,headline_t.text.strip(),str(div3.text).strip(),today])\n",
    "        df_comments = pd.DataFrame(comments_list, columns=['URLs','headline', 'text', 'Retrived_date'])\n",
    "        df_comments.insert(2,'article_date',value=time_list[0])\n",
    "        process_data = create_ListofDict_InsertMany(df_comments)\n",
    "        print(process_data)\n",
    "        col_comment.insert_many(process_data)\n",
    "        del df_comments\n",
    "        comments_list.clear()\n",
    "        time_list.clear()\n",
    "    except Exception as err:\n",
    "        print('กระทู้ไม่ถูกดึงเข้าระบบ', err)\n",
    "        error_list.append([cnt,au,err])\n",
    "    time.sleep(postNumSleep)\n",
    "    driver.quit()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(error_list)\n",
    "#  URL Error\n",
    "# 1. 1069  Too long and unrelevent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
