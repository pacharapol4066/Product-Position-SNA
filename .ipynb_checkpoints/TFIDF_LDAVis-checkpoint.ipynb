{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoY8sy40TCb5"
   },
   "source": [
    "### IS - Brand position\n",
    "Finding Product position on Social Network (PPSN)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28353,
     "status": "ok",
     "timestamp": 1604024868503,
     "user": {
      "displayName": "Pacharapol O.",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiBNaVjsCxoMVUqEvSDC7eauF0V2ZD7B-TzevR1TQ=s64",
      "userId": "00378494490135533545"
     },
     "user_tz": -420
    },
    "id": "GkBEUBUqTGuf",
    "outputId": "cb22db45-820a-48fd-c11a-a0568439778f"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1693,
     "status": "ok",
     "timestamp": 1604024868819,
     "user": {
      "displayName": "Pacharapol O.",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiBNaVjsCxoMVUqEvSDC7eauF0V2ZD7B-TzevR1TQ=s64",
      "userId": "00378494490135533545"
     },
     "user_tz": -420
    },
    "id": "_5mrzpx3TH9K",
    "outputId": "129bb192-4ac2-4b03-bc89-7ee7a65d5032"
   },
   "source": [
    "cd /content/drive/My\\ Drive/Colab\\ Notebooks/Master_PJ_DRMABS/Datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2631,
     "status": "ok",
     "timestamp": 1604024879769,
     "user": {
      "displayName": "Pacharapol O.",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiBNaVjsCxoMVUqEvSDC7eauF0V2ZD7B-TzevR1TQ=s64",
      "userId": "00378494490135533545"
     },
     "user_tz": -420
    },
    "id": "cC0cBdCdTCb6"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import math\n",
    "import string\n",
    "import json\n",
    "import pymongo\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.core.common.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2031,
     "status": "ok",
     "timestamp": 1604024879770,
     "user": {
      "displayName": "Pacharapol O.",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiBNaVjsCxoMVUqEvSDC7eauF0V2ZD7B-TzevR1TQ=s64",
      "userId": "00378494490135533545"
     },
     "user_tz": -420
    },
    "id": "1kqXURiiTCb-"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "today = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3451,
     "status": "ok",
     "timestamp": 1604028167896,
     "user": {
      "displayName": "Pacharapol O.",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiBNaVjsCxoMVUqEvSDC7eauF0V2ZD7B-TzevR1TQ=s64",
      "userId": "00378494490135533545"
     },
     "user_tz": -420
    },
    "id": "weEcXSD8TCcI",
    "outputId": "1219680e-0d54-4649-a26e-535e112f0650"
   },
   "outputs": [],
   "source": [
    "#!pip install pythainlp\n",
    "import pythainlp\n",
    "from pythainlp import Tokenizer\n",
    "from pythainlp.util import normalize\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus.common import thai_words\n",
    "from pythainlp import sent_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from pythainlp.util import dict_trie\n",
    "from pythainlp.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jpa16DrfTCeL"
   },
   "outputs": [],
   "source": [
    "# https://thainlp.org/pythainlp/docs/2.0/api/tag.html\n",
    "from pythainlp.spell import correct\n",
    "from pythainlp.tag import pos_tag\n",
    "\n",
    "#https://thainlp.org/pythainlp/docs/2.0/api/transliterate.html\n",
    "from pythainlp.transliterate import romanize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as stat\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6121,
     "status": "ok",
     "timestamp": 1604024888335,
     "user": {
      "displayName": "Pacharapol O.",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiBNaVjsCxoMVUqEvSDC7eauF0V2ZD7B-TzevR1TQ=s64",
      "userId": "00378494490135533545"
     },
     "user_tz": -420
    },
    "id": "acWWFhczTCcO"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5596,
     "status": "ok",
     "timestamp": 1604024888336,
     "user": {
      "displayName": "Pacharapol O.",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiBNaVjsCxoMVUqEvSDC7eauF0V2ZD7B-TzevR1TQ=s64",
      "userId": "00378494490135533545"
     },
     "user_tz": -420
    },
    "id": "l0SZe4-LTCcd"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1604027698593,
     "user": {
      "displayName": "Pacharapol O.",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiBNaVjsCxoMVUqEvSDC7eauF0V2ZD7B-TzevR1TQ=s64",
      "userId": "00378494490135533545"
     },
     "user_tz": -420
    },
    "id": "sna_e7nATCch"
   },
   "outputs": [],
   "source": [
    "# Files definition\n",
    "root_path = 'Datasource'\n",
    "\n",
    "comment_file = []\n",
    "comment_file.append(root_path+\"/comment_meiji.csv\")\n",
    "comment_file.append(root_path+\"/comment_DDCF.csv\")\n",
    "\n",
    "#comment_file = []\n",
    "#comment_file.append(\"comment_meiji.csv\")\n",
    "#comment_file.append(\"comment_DDCF.csv\")\n",
    "\n",
    "comment_nlp_file = root_path+\"/comment_nlpToken.csv\"\n",
    "comment_cooc_lift_file = root_path+\"/comment_cooc_Lift.xlsx\"\n",
    "\n",
    "#comment_nlp_file = \"comment_nlpToken.csv\"\n",
    "#comment_cooc_nlp_file = \"comment_cooc.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect MongoDB\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "mydb = myclient[\"NIDA_PPSN_PRD\"]\n",
    "col_thread = mydb[\"NIDA_PPSN_THREAD\"]\n",
    "col_comment = mydb[\"NIDA_PPSN_COMMENT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Process หัวกระทู้เพื่อดึง Domain (Brand & Product) ไม่ใช้โมเดลเทรนแล้วทำนาย ต้องการความถูกต้อง 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor_thread = col_thread.find()\n",
    "df_thr_process = pd.DataFrame(cursor_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor_comment = col_comment.find()\n",
    "df_cmt_process = pd.DataFrame(cursor_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls_milk = df_thr_process[(df_thr_process.t_mention_brand.str.len() != 0) | (df_thr_process.t_mention_product.str.len() != 0)]\n",
    "df_urls_milk['t_mention_brand'] = df_urls_milk['t_mention_brand'].apply(lambda x: repr(set(x)))\n",
    "df_urls_milk['t_mention_product'] = df_urls_milk['t_mention_product'].apply(lambda x: repr(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_url = pd.merge(df_urls_milk,df_cmt_process,how='inner',on='URLs')\n",
    "df_join_url.drop(columns=['Table','_id_x','_id_y','text_x','token_text_x','cmt_mention_brand','cmt_mention_product'],inplace=True)\n",
    "df_join_url.columns = ['URLs','t_mention_brand','t_mention_product'\n",
    "                       ,'Retrived_date','commentId','comment_text','token_text',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_url.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_view = df_urls_milk[['URLs','t_mention_product']]\n",
    "df_view.column = ['URLs','t_mention_product']\n",
    "df_view_count = df_view.groupby('t_mention_product').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c_urls_milk = df_cmt_process[(df_cmt_process.cmt_mention_brand.str.len() != 0) | (df_cmt_process.cmt_mention_product.str.len() != 0)]\n",
    "df_c_urls_milk['cmt_mention_brand'] = df_c_urls_milk['cmt_mention_brand'].apply(lambda x: repr(set(x)))\n",
    "df_c_urls_milk['cmt_mention_product'] = df_c_urls_milk['cmt_mention_product'].apply(lambda x: repr(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_view2 = df_c_urls_milk[['URLs','cmt_mention_product']]\n",
    "df_view2.column = ['URLs','cmt_mention_product']\n",
    "df_view_count2 = df_view2.groupby('cmt_mention_product').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('จำนวนกระทู้',df_thr_process.shape[0])\n",
    "print('จำนวน comment',df_cmt_process.shape[0])\n",
    "print('จำนวนกระทู้พูดถึง brand หรือ มีนมเกี่ยวข้อง อย่างใดอย่างหนึ่ง',df_urls_milk.shape[0])\n",
    "print('จำนวนกระทู้มีระบุตัว product',df_view[(df_view.t_mention_product!=\"{'นม'}\")&(df_view.t_mention_product!=\"set()\")].shape[0])\n",
    "print('จำนวน comment พูดถึง brand หรือมีนมเกี่ยวข้องง อย่างใดอย่างหนึ่ง',df_c_urls_milk.shape[0])\n",
    "print('จำนวน comment มีระบุตัว product',df_view2[(df_view2.cmt_mention_product!=\"{'นม'}\")&(df_view2.cmt_mention_product!=\"set()\")].shape[0])\n",
    "print('odd product/นม กระทู้',round(df_view[(df_view.t_mention_product!=\"{'นม'}\")&(df_view.t_mention_product!=\"set()\")].shape[0]/df_urls_milk.shape[0],2))\n",
    "print('odd product/นม comment',round(df_view2[(df_view2.cmt_mention_product!=\"{'นม'}\")&(df_view2.cmt_mention_product!=\"set()\")].shape[0]/df_c_urls_milk.shape[0],2))\n",
    "print('หมายเหตุ : ไม่ได้ impute ขั้วกระทู้เข้า comment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จาก Issue นี้จะพบว่าทำ Product position ได้ยาก แต่ทำ Brand position แทนได้ เนื่องจากสัดส่วนของข้อความที่มีการพูดถึง brand+flavor \n",
    "มีน้อยกว่าครึ่ง จึงให้ใช้วิธีเอา brand ของหัวกระทู้แปะไปในทุก comment แทน เพื่อทำ Brand Position ส่วน flavor จะกลายเป็น attribute ไป"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_view_count2.sort_values(by='URLs',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4. คัดเลือกคำมาเป็น Attribute ด้วย TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.สรุป Term & reduced_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrributes = ['การบูด','โปร','โปรโมชั่น','แถม','บูด','เป็นผลดี','หา','หมดอายุ', 'กลิ่น','หอม','กลิ่นหอม', 'แนะนำ', 'กิน', 'shelf life','แม่','คุณแม่'\n",
    " ,'ลูก','เด็ก','เลี้ยงลูก','เลี้ยงเด็ก','พัฒนาการ','คุณแม่มือใหม่','สั่ง', 'ค่าใช้จ่าย', 'เพิ่มขึ้น', 'สินค้า', 'ชอบ','นิยม', 'สี', 'ซื้อไม่ได้'\n",
    " , 'ทิ้ง', 'จืด','หวาน' , 'fat', 'นิยม','ราคาสูง','กาแฟ','ชงกาแฟ','หาย','ต้ม','อุ่น','ร้อน', 'พื้นฐาน', 'ขายไม่ดี'\n",
    " , 'กาแฟสด', 'จ่าย', 'ไขมันต่ำ', 'นม', 'ขาย', 'รส','รสชาติ', 'รสหวาน','กลมกล่อม','ไขมัน','พนักงาน','ไมโครเวฟ','ฟาร์ม'\n",
    " ,'ส่วนผสม','ผสม','เบาหวาน','รสพื้นฐาน','โปรตีน','เวย์','เวท','วิ่ง','protein','whey','ขับถ่าย','อึ','dha','น้ำตาล','เบาหวาน'\n",
    " ,'ฝา', 'นมสด','ถ้วย', 'ร้าน', 'ดื่ม', 'เซเว่น', 'ขวด', 'นึกถึง', 'เสียใจ', 'แตก', 'ซื้อ','ขาย','มีลูก','เทรนเนอร์'\n",
    " ,'ขายไม่ดี','แพคคู่','ค่าจัดส่ง','shelf life','พนักงานขายนม','แพ้นม','แพ้นมวัว','เล่นเวท','ฝาน้ำเงิน','ฝาสีเขียว'\n",
    " ,'นมอุ่น','ชานม','กินนม','ดื่มนม','ท้องเสีย','ลูกสุนัข','สุนัข','หมา','แมว','คายทิ้ง','เจมส์จิ','สตอรี่','แป้ง','ลดราคา'\n",
    " ,'ซื้อประจำ','ซื้อไม่ได้','คาปูชิโน่','อเมริกาโน่','ร้านนม','whey formula','ผิดสังเกต','เสียความรู้สึก','ชี้แจง','บำรุง','น้ำผึ้ง'\n",
    " ,'อาหารเสริม','มีประโยชน์','วิตามิน','นมผง','แคลเซี่ยม','ท้อง','แพะ','อร่อย','โภชนาการ','โรงเรียน','พ่อแม่','ครู','ยูเอชที'\n",
    " ,'ผู้บริโภค','ขโมย','ไอโอดีน','นมข้นหวาน','เนย','ตรวจสอบ','แกลลอน','มันดี','นมวัว','อาเจียน','เวฟ','ไมโครเวฟ','สารอาหาร'\n",
    " ,'ขนส่ง','ถูก','แพง','เค้ก','ทิ้ง','วันหมดอายุ','โอเมก้า','กล่อง','พลังงาน','โภชนาการ','ขนมปัง','ของแถม','ราคาสูง','น้ำนมโค'\n",
    " ,'บรรจุภัณฑ์','นมถั่วเหลือง','ความแข็งแรง','แข็งแรง','พรีเซ็นเตอร์','ญี่ปุ่น','น้ำผลไม้','ออกกำลังกาย','ประหยัด','วิปปิ้ง','uht','เชื้อจุลินทรีย์'\n",
    " ,'แลคโตส']\n",
    "\n",
    "products = ['สตรอว์เบอร์รี','ช็อกโกแลต','กาแฟ','รสหวาน','รสจืด','รสกาแฟ','ไขมันต่ำ','ไขมัน 0%','ไฮโปรตีน','อัลมอนด์'\n",
    ",'รสกล้วย','grass fed','นมฟรีแลคโตส','พาสเจอร์ไรส์','เมจิโกลด์','นมฮอกไกโด','เบดไทม์','ดาร์คช็อกโกแลต','ไฮแคลเซียม'\n",
    ",'คาราเมล','มอลต์','เมล่อน','ชาเขียวมัจฉะ','บัลแกเรีย','โยเกิร์ต','รสธรรมชาติ','รสกลมกล่อม','โยเกิร์ต','ซากุระ','โคล่า'\n",
    ",'แตงโม']\n",
    "\n",
    "brands = ['ดัชมิลล์','เมจิ','โฟร์โมสต์','โชคชัย','แดรี่โฮม','เอ็มมิลค์','แมคโนเลีย','ไทยเดนมาร์ค'\n",
    "          ,'หนองโพ','คาเนชั่น','บีทาเก้น','จิตรลดา']\n",
    "stores = ['tops','makro','lotus','bigc','7-Eleven']\n",
    "\n",
    "reduceCol = attrributes + products + stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reduceCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduced_keyword(wtkn,redCol):\n",
    "    del_list = []\n",
    "    ret_wtkn = wtkn\n",
    "    for each in ret_wtkn:\n",
    "        if each not in redCol:\n",
    "            del_list.append(each)\n",
    "    ret_wtkn = [x for x in ret_wtkn if x not in del_list]\n",
    "    return ret_wtkn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKW--A73mzli"
   },
   "source": [
    "### 6.สร้าง (Reduce) Bag of word ด้วย dictionary.doc2bow จัดลง dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor_thread = col_thread.find()\n",
    "df_thr_process = pd.DataFrame(cursor_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor_comment = col_comment.find()\n",
    "df_cmt_process = pd.DataFrame(cursor_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls_milk = df_thr_process[(df_thr_process.t_mention_brand.str.len() != 0) | (df_thr_process.t_mention_product.str.len() != 0)]\n",
    "df_urls_milk['t_mention_brand'] = df_urls_milk['t_mention_brand'].apply(lambda x: repr(x))\n",
    "df_urls_milk['t_mention_product'] = df_urls_milk['t_mention_product'].apply(lambda x: repr(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_url = pd.merge(df_urls_milk,df_cmt_process,how='inner',on='URLs')\n",
    "df_join_url.drop(columns=['Table','_id_x','_id_y','text_x','token_text_x','cmt_mention_brand','cmt_mention_product'],inplace=True)\n",
    "df_join_url.columns = ['URLs','t_mention_product','t_mention_brand','commentId'\n",
    "                       ,'comment_text','Retrived_date','token_text',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_url['token_text_reduce'] = df_join_url['token_text'].apply(lambda x: reduced_keyword(x,reduceCol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_join_url.set_index('commentId',inplace=True)\n",
    "df_join_url.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(df_join_url['token_text_reduce'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_url['reduce_bow'] = df_join_url[\"token_text_reduce\"].map(dictionary.doc2bow)\n",
    "df_join_url['reduce_bow_txt'] = df_join_url[\"reduce_bow\"].apply(lambda x:[(dictionary[id_], frequence) for id_, frequence in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def freq_brand(x):\n",
    "    e = eval(x)\n",
    "    e.sort()\n",
    "    f = [(k,len(list(g))) for k, g in groupby(e)]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_url['t_mention_brand'] = df_join_url['t_mention_brand'].apply(lambda x: freq_brand(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_join_url.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brand_cooc = pd.concat([pd.DataFrame(s,columns=['w','c']).set_index('w') for s in df_join_url[\"t_mention_brand\"]], axis=1, sort=False).fillna(0).T.set_index(df_join_url.index)\n",
    "df_cooc = pd.concat([pd.DataFrame(s,columns=['w','c']).set_index('w') for s in df_join_url[\"reduce_bow_txt\"]], axis=1, sort=False).fillna(0).T.set_index(df_join_url.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brand_cooc.reset_index(inplace=True)\n",
    "df_cooc.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = pd.merge(df_brand_cooc, df_cooc, on='commentId', how='inner')\n",
    "join_df.set_index('commentId',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBBA36fDTCfZ"
   },
   "source": [
    "### 7.Create frequency co-occurrence matrix จาก Bag of word"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "LgE1okejTCfZ"
   },
   "source": [
    "item_item_matrix = pd.DataFrame(index=join_df.columns,columns=join_df.columns).fillna(0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "05j3U3ymTCfb"
   },
   "source": [
    "def cnt_Grpby(df,i,j):\n",
    "    PAIB = pd.DataFrame(df.groupby(df.iloc[:,i].name)[df.iloc[:,j].name].value_counts())\n",
    "    PAIB.index.names = ['A','B']\n",
    "    PAIB.columns = ['freq']\n",
    "    PAIB.reset_index(inplace=True)\n",
    "    _PAIB = PAIB[(PAIB.A>0)&(PAIB.B>0)]\n",
    "    return _PAIB.freq.sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "1jhFte5eTCfd"
   },
   "source": [
    "# Co-occurrence\n",
    "for i in range(0,len(item_item_matrix.columns)):\n",
    "    for j in range(0,len(item_item_matrix.columns)):\n",
    "        if i != j:\n",
    "            item_item_matrix.iloc[i,j] = cnt_Grpby(join_df,i,j)\n",
    "        else:\n",
    "            item_item_matrix.iloc[i,j] = 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "OhtvLfMXTCfg"
   },
   "source": [
    "item_item_matrix.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "bsbsqduDTCfh"
   },
   "source": [
    "item_item_matrix.to_excel(comment_cooc_nlp_file, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8nlNNE4TCfj"
   },
   "source": [
    "### 8.Create co-occurrence matrix with Lift normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qykd9Z88TCfj"
   },
   "outputs": [],
   "source": [
    "item_item_matrix = pd.DataFrame(index=join_df.columns,columns=join_df.columns).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElfF27atTCfp"
   },
   "outputs": [],
   "source": [
    "def a_in_b(df,i,j):\n",
    "    PAIB = pd.DataFrame(df.groupby(df.iloc[:,j].name)[df.iloc[:,i].name].value_counts())\n",
    "    PAIB.index.names = ['B','A']\n",
    "    PAIB.columns = ['freq']\n",
    "    PAIB.reset_index(inplace=True)\n",
    "    PAIB_ = PAIB[(PAIB.A>0)&(PAIB.B>0)]\n",
    "    return PAIB_.freq.sum()/PAIB.freq.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1sDGAMWTCfr"
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(item_item_matrix.columns)) :\n",
    "    for j in range(0,len(item_item_matrix.columns)) :\n",
    "        A = join_df.iloc[:,i]\n",
    "        B = join_df.iloc[:,j]\n",
    "        PA = A[A!=0].count()/A.shape[0]\n",
    "        PB = B[B!=0].count()/B.shape[0]\n",
    "        PAB = PB*a_in_b(join_df,i,j)\n",
    "        item_item_matrix.iloc[i,j] = PAB/(PA*PB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wzd336KbTCfs"
   },
   "outputs": [],
   "source": [
    "item_item_matrix.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "item_item_matrix.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qG7pCYfTCfu"
   },
   "outputs": [],
   "source": [
    "item_item_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LsZZMOcTTCfw"
   },
   "outputs": [],
   "source": [
    "item_item_matrix.to_excel(comment_cooc_lift_file, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.ทำ Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Domain ที่พบหลักๆ นอกเหนือจากตัว Product ห้างที่ขาย แบรนด์</b><br>\n",
    "ค้นหาเทียบเคียงกับ ปัจจัยที่มีผลต่อการบริโภคนม\n",
    "1. แม่และเด็ก (Kurajdova et al, 2015)\n",
    "2. ออกกำลังกาย เล่นเวท วิ่ง พูดถึง Whey หางนม โปรตีน เป็นหลัก (คิดว่าแตกจากหมวดสุขภาพ)\n",
    "3. โปรโมชั่น เช่น จัดโปร ส่วนลด พันแถม แพ๊กคู่ (สุภชาติ ชัยณรงค์สิงห์, 2539)\n",
    "4. เอานมให้หมาแมวกิน (เทรนด์ใหม่)\n",
    "5. รสชาติและคุณภาพ เช่น หอม เหม็น มัน อร่อย บูด เสีย GMP (ชาลิสา สถีระกานนท์, 2560)\n",
    "6. ความยากง่ายในการหาซื้อ (Aen De Alwis et al, 2009 & สุภชาติ ชัยณรงค์สิงห์, 2539)\n",
    "7. ทำกาแฟ ทำขนม (Kurajdova et al, 2015)\n",
    "8. สุขภาพ ความสูง ส่วนผสมเพื่อสุขภาพ การขับถ่าย อนุมูลอิสระ (Kurajdova et al, 2015 & Aen De Alwis et al, 2009 & สุภชาติ ชัยณรงค์สิงห์, 2539)\n",
    "9. ราคาและความคุ้มค่า (ปิยฉัตร ช่างเหล็ก, 2561)\n",
    "<br><br>ตรงนี้ต้องหา Paper มาช่วยยันในบางหัวข้อ ถ้ามีส่วนเกินถือว่าเป็นบริบทของประเทศไทย"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 สร้าง Bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(df_join_url['token_text'])\n",
    "gensim_corpus = [dictionary.doc2bow(text, allow_update=True) for text in df_join_url['token_text']]\n",
    "word_frequencies = [[(dictionary[id], frequence) for id, frequence in couple] for couple in gensim_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 หา Optimal Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 สร้าง LdaModel ตามกลุ่มที่หาได้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_topics = 9\n",
    "chunksize = 4000                  # size of the doc looked at every pass\n",
    "iterations = 50\n",
    "eval_every = 1                    # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "%time model = gensim.models.LdaModel(corpus=gensim_corpus, id2word=id2word, chunksize=chunksize \\\n",
    "                                     ,alpha='auto', eta='auto',iterations=iterations, eval_every=eval_every)\n",
    "                                     #,num_topics=num_topics \\\n",
    "                                     #, eval_every=eval_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(model, gensim_corpus, dictionary, R=40, lambda_step=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Get the most relevant topics to the given word.\n",
    "# Set probability = 0.0001\n",
    "\n",
    "def get_terms_topics(term_text, d, model, minProb=0.0001):\n",
    "    listofTup = []\n",
    "    for tt_couple in model.get_term_topics(dictionary.token2id[term_text], minimum_probability=minProb):\n",
    "        id_, prob = tt_couple\n",
    "        listofTup.append((d[id_], prob))\n",
    "    df = pd.DataFrame(listofTup)\n",
    "    df.insert(0,'Term',value=term_text)\n",
    "    df.columns = ['Term','topic','prob']\n",
    "    return df.sort_values(by='prob', ascending=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Get the most relevant topics to the given word.\n",
    "# Set probability = 0.0001\n",
    "\n",
    "def get_topics_terms(topic, d, model, minProb=0.0001):\n",
    "    listofTup = []\n",
    "    for tt_couple in model.get_topic_terms(topic,topn=30):\n",
    "        id_, prob = tt_couple\n",
    "        listofTup.append((d[id_], prob))\n",
    "    df = pd.DataFrame(listofTup)\n",
    "    df.insert(0,'Topic',value=d[topic])\n",
    "    df.columns = ['Topic','term','prob']\n",
    "    return df.sort_values(by='prob', ascending=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# องค์ประกอบพื้นฐาน ตาม master ที่ใช้ตัดคำ และ Generalized\n",
    "#get_terms_topics('ซื้อ',dictionary,model)\n",
    "#get_terms_topics('ขาย',dictionary,model)\n",
    "\n",
    "# ไล่เช็คทีละ Topic โดยเลือกจาก Top30 Prob แต่ละกลุ่ม และดูตัวที่สนใจ (ตามทบทวนวรรณกรรม)\n",
    "# สุดที่ 99 กลุ่ม\n",
    "#get_topics_terms(99,dictionary,model).head(3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "EpwX95XlTCdl",
    "_Q9pU1J6TCe5",
    "NKW--A73mzli",
    "uBBA36fDTCfZ",
    "O8nlNNE4TCfj"
   ],
   "name": "EDA_Token_CoocMat.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
